{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52de63e9-e3be-4ad5-8f30-1ccb2e8facc6",
   "metadata": {},
   "source": [
    "### 4.Implement word embeddings for IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3589988a-4fef-4f1c-aa13-d7948f7b38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e9ef1f1-5d9a-4f1e-8ada-5d27704412df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load IMDB data\n",
    "max_words = 10000\n",
    "max_len = 200\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02c8b2e-4ad3-43ec-84ef-db0ef2dfce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load GloVe word embeddings (glove.6B.50d.txt must be in your directory)\n",
    "embeddings_index = {}\n",
    "with open(\"glove.6B.50d.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a67aee08-cee9-4b75-a28d-91f2a77a892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare embedding matrix\n",
    "word_index = imdb.get_word_index()\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b373aaf-671e-4580-b791-3a005e7a60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71ba1e32-0b1b-4eac-bc42-b01757dba82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akanksh_02\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, weights=[embedding_matrix],\n",
    "                    input_length=max_len, trainable=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04e06df-7462-40bf-bb8f-edbec8b4a619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6566 - loss: 0.6018 - val_accuracy: 0.8318 - val_loss: 0.3771\n",
      "Epoch 2/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8930 - loss: 0.2737 - val_accuracy: 0.8440 - val_loss: 0.3620\n",
      "Epoch 3/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9451 - loss: 0.1567 - val_accuracy: 0.8468 - val_loss: 0.3984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x245b1d3ca40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Compile and train\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b561d151-2431-4550-b1c1-27ab4cf13f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 872us/step - accuracy: 0.8464 - loss: 0.4009\n",
      "Test Accuracy: 84.68%\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38e7f962-febe-414b-9492-74892638532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings saved to updated_glove_embeddings.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save updated word embeddings\n",
    "updated_embeddings = model.layers[0].get_weights()[0]\n",
    "inverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "with open(\"updated_glove_embeddings.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(1, max_words):  # skipping 0 (padding)\n",
    "        word = inverse_word_index.get(i, \"<UNK>\")\n",
    "        vector = updated_embeddings[i]\n",
    "        vector_str = \" \".join(map(str, vector))\n",
    "        f.write(f\"{word} {vector_str}\\n\")\n",
    "\n",
    "print(\"Word embeddings saved to updated_glove_embeddings.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
