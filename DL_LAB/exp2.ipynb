{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9dc205-9b18-4a8d-a422-b195d1816a17",
   "metadata": {},
   "source": [
    "### Implement one hot encoding of words or characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0735d944-5627-456f-8b3b-708248870492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2e2c2e3-24ca-464c-8ae0-3c68e81ee87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2f78558-17cd-4005-ba4d-3d99405a35ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'The': 0, 'cat': 1, 'sat': 2, 'on': 3, 'the': 4, 'mat.': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework.': 9}\n",
      "Word-Level One-Hot:\n",
      " [[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#Part 1: Word-Level One-Hot Encoding (Manual Method)\n",
    "# Create token index\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index)\n",
    "\n",
    "# Define max length for padding/truncation\n",
    "max_length = 10\n",
    "\n",
    "# Initialize results array\n",
    "results_word = np.zeros((len(samples), max_length, len(token_index)))\n",
    "\n",
    "# Fill in one-hot encodings\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results_word[i, j, index] = 1\n",
    "\n",
    "print(\"Word Index:\", token_index)\n",
    "print(\"Word-Level One-Hot:\\n\", results_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71250e79-52ea-444e-9c53-5dbf9e0d27a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Index: {' ': 0, '.': 1, 'T': 2, 'a': 3, 'c': 4, 'd': 5, 'e': 6, 'g': 7, 'h': 8, 'k': 9, 'm': 10, 'n': 11, 'o': 12, 'r': 13, 's': 14, 't': 15, 'w': 16, 'y': 17}\n",
      "Character-Level One-Hot:\n",
      " [[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#Part 2: Character-Level One-Hot Encoding (Manual Method)\n",
    "# Get all unique characters\n",
    "all_text = ''.join(samples)\n",
    "characters = sorted(list(set(all_text)))\n",
    "char_index = dict(zip(characters, range(len(characters))))\n",
    "\n",
    "# Max character length per sentence\n",
    "max_length_char = max(len(sample) for sample in samples)\n",
    "\n",
    "# Initialize results array\n",
    "results_char = np.zeros((len(samples), max_length_char, len(characters)))\n",
    "\n",
    "# Fill in one-hot encodings\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, char in enumerate(sample[:max_length_char]):\n",
    "        index = char_index.get(char)\n",
    "        results_char[i, j, index] = 1\n",
    "\n",
    "print(\"Character Index:\", char_index)\n",
    "print(\"Character-Level One-Hot:\\n\", results_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b41e5fee-a4b4-4395-ae5e-a54169eb336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "Sample 2:\n",
      "0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Part 3: Save Character-Level Results to File\n",
    "file_path = 'C:\\\\Users\\\\akanksh_02\\\\DL_LAB\\\\output_ex2.txt'\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "    for i, sample_vector in enumerate(results_char):\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        for vector in sample_vector:\n",
    "            f.write(' '.join(map(str, vector.astype(int))) + '\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "946e8c9a-ea92-46ae-8d1e-74664c72c485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1 One-Hot Encoding:\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "Sentence 2 One-Hot Encoding:\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Part 4: Manual Word-Level One-Hot Encoding (Per Word)\n",
    "\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index)\n",
    "\n",
    "results_list = []\n",
    "for sample in samples:\n",
    "    sentence_encoding = []\n",
    "    for word in sample.split():\n",
    "        one_hot_vector = np.zeros(len(token_index))\n",
    "        one_hot_vector[token_index[word]] = 1\n",
    "        sentence_encoding.append(one_hot_vector)\n",
    "    results_list.append(sentence_encoding)\n",
    "\n",
    "# Display results\n",
    "for i, sentence in enumerate(results_list):\n",
    "    print(f\"\\nSentence {i+1} One-Hot Encoding:\")\n",
    "    for word_vec in sentence:\n",
    "        print(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b391abf-626b-40ac-bb7b-417530b08730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Word Index: {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n",
      "Found 9 unique words.\n",
      "Tokenizer One-Hot Matrix:\n",
      " [[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Part 5: One-Hot Encoding Using Keras Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)  # Keep top 1000 words\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Tokenizer Word Index:\", word_index)\n",
    "print(\"Found\", len(word_index), \"unique words.\")\n",
    "print(\"Tokenizer One-Hot Matrix:\\n\", one_hot_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd428b-8cdb-49dc-a999-672eb7e971af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78fb2e0-b957-4989-bcba-1a2d338c4327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
